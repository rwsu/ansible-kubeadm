- hosts: all
  remote_user: root
  tasks:
    - name: install docker
      yum:
        name: docker
        state: installed

    - name: ensure docker is running
      service:
        name: docker
        state: started
        enabled: yes

    - name: firewall status
      service:
        name: firewalld
        state: stopped
#        enabled: yes

    - name: add kubernetes.repo
      yum_repository:
        name: kubernetes
        description: upstream kubernetes repo
        baseurl: https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
        enabled: yes
        gpgcheck: yes
        repo_gpgcheck: yes
        gpgkey: https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg

    # SELinux needs to be disabled to allow containers to access the host filesystem as
    # required by pod networks. is this really needed??
    - name: install python2-libselinux
      yum:
        name: python2-libselinux
        state: installed

    - name: set SELinux to permissive
      selinux:
        policy: targeted
        state: permissive

#    - name: install kubelet kubeadm kubectl with version
#      command: yum install -y kubelet-{{ kubernetes_version }} kubeadm-{{ kubernetes_version }} kubectl-{{ kubernetes_version }}
#      when: kubernetes_version is defined

    - name: install kubelet kubeadm kubectl
      command: yum install -y kubelet kubeadm kubectl

    - name: systemctl daemon-reload
      command: systemctl daemon-reload

    - name: ensure kubelet is running
      service:
        name: kubelet
        state: started
        enabled: yes

    - name: disable swap
      command: swapoff -a

    - name: sysctl net.bridge.bridge-nf-call-iptables=1
      sysctl:
        name: net.bridge.bridge-nf-call-iptables
        value: 1
        sysctl_set: yes
        state: present
        reload: yes

- hosts: master
  remote_user: root
  tasks:

  # Open Master node TCP ports (inbound)
  # 6443      Kubernetes API server
  # 2379-2380 etcd server client API
  # 10250     Kubelet API
  # 10251     kube-scheduler
  # 10252     kube-controller-manager
  # 10255     Read-only Kubelet API
    - iptables:
        table: filter
        chain: INPUT
        action: insert
        protocol: tcp
        match: tcp
        ctstate: NEW
        jump: ACCEPT
        destination_port: "{{ item }}"
      become: true
      with_items:
        - 6443
        - 2379:2380
        - 10250
        - 10251
        - 10252
        - 10255

#    - name: extra packages
#      yum:
#        name: ansible libvirt-client
#        state: installed

#    - name: run kubeadm init with version
#      shell: kubeadm init --kubernetes-version "{{ kubernetes_version }}" --pod-network-cidr=10.244.0.0/16 | grep "kubeadm join"
#      register: kubeadm_join_command
#      creates: /etc/kubernetes/admin.conf
#      when: kubernetes_version is defined

    - name: run kubeadm init
      shell: kubeadm init --pod-network-cidr=10.244.0.0/16 | grep "kubeadm join"
      register: kubeadm_join_command
      args:
        creates: /etc/kubernetes/admin.conf

    - debug:
        var: kubeadm_join_command.stdout_lines

    - name: set join command as fact
      set_fact: joincommand="{{kubeadm_join_command.stdout}}"

    - name: wait for kube-controller to have Running status
      shell: kubectl get pods --kubeconfig /etc/kubernetes/admin.conf --all-namespaces | grep kube-controller
      register: kube_controller_status
      until: kube_controller_status.stdout.find("Running") != -1
      retries: 30
      delay: 10

    - name: copy /etc/kubernetes/admin.conf to ~/.kube/config
      command: cp /etc/kubernetes/admin.conf ~/.kube/config

#    - name: install Flannel CNI
##      command: kubectl apply --kubeconfig /etc/kubernetes/admin.conf -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml
#      command: kubectl apply --kubeconfig /etc/kubernetes/admin.conf -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
#      register: cni_install

    - name: install Weave Net CNI
      shell: export kubever=$(kubectl version | base64 | tr -d '\n'); kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$kubever"
      register: cni_install

    - name: wait for kube-dns to have Running status
      shell: kubectl get pods --kubeconfig /etc/kubernetes/admin.conf --all-namespaces | grep dns
      register: kube_dns_status
      until: kube_dns_status.stdout.find("Running") != -1
      retries: 30
      delay: 10

    - name: allow master to schedule pods. needed for gluster 3-node requirement
      command: kubectl taint nodes --all node-role.kubernetes.io/master-

    - name: add dashboard
      command: kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml

    - name: copy admin user yaml
      copy:
        src: dashboard-user.yml
        dest: /root

    - name: create dashboard admin user
      command: kubectl create -f dashboard-user.yml

    # To get token for dashboard
    # kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')

    - name: dashboard user
      shell: kubectl -n kube-system get secret | grep admin-user | awk '{print $1}'
      register: dashboard_user

    - debug: var=dashboard_user

    - name: dashboard token
      command: "kubectl -n kube-system describe secret {{ dashboard_user.stdout }}"
      register: dashboard_token

    - debug: var=dashboard_token

- hosts: worker
  remote_user: root
  tasks:

  # Open Worker node TCP ports (inbound)
  # 10250       Kubelet API
  # 10255       Read-only Kubelet API
  # 30000-32767 NodePort Services

    - iptables:
        table: filter
        chain: INPUT
        action: insert
        protocol: tcp
        match: tcp
        ctstate: NEW
        jump: ACCEPT
        destination_port: "{{ item }}"
      become: true
      with_items:
        - 10250
        - 10255
        - 30000:32767

    - name: join worker node
      command: "{{ hostvars[groups['master'][0]]['joincommand'] }}"
